{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba89df43",
   "metadata": {},
   "source": [
    "# An Introduction to the Ray AI Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900de5b",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab TODO](https://colab.research.google.com/github/XXX).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/XXX\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "TODO: Make this 2.5 or 2.6 later.\n",
    "The book has been written for Ray 2.4.0, which you can install using `pip install ray==2.4.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f0182",
   "metadata": {},
   "source": [
    "To run the examples for this chapter, you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8477db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO pin all versions here\n",
    "! pip install \"ray[air]>=2.4.0\" \"accelerate>=0.16.0\" \"transformers>=4.26.0\"\n",
    "! pip install \"numpy<1.24\" \"torch>=1.12.0\" \"datasets\" \"evaluate\" \"deepspeed\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "In this chapter we’ll introduce you to the core concepts of the Ray AI Runtime (AIR) and how you can\n",
    "use it to build and deploy common ML workflows. To introduce its components we’ll build\n",
    "an AIR application that fine-tunes an open-source language model, deploys it for online\n",
    "inference and uses the model for offline batch inference.\n",
    "We will also tell you when and why to use AIR and give you a brief overview of its ecosystem.\n",
    "We close with an in-depth discussion of the relationship of AIR with other systems."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why and When to Use AIR?\n",
    "\n",
    "Running ML workloads with Ray has been a constant evolution over the last couple\n",
    "of years. Ray RLlib and Tune were the first libraries built on top of Ray Core.\n",
    "Components like Ray Train, Serve, and more recently Datasets followed shortly\n",
    "after. The addition of Ray AIR as an umbrella for all other Ray ML libraries is the\n",
    "result of active discussions with and feedback from the ML community. Ray, as a\n",
    "Python-native tool with good GPU support and stateful primitives (Ray actors) for\n",
    "complex ML workloads, is a natural candidate for building a runtime like AIR."
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ray AIR is a unified toolkit for your ML workloads that offers many third-party\n",
    "integrations for model training or accessing custom data sources. In the spirit of the\n",
    "other ML libraries built on top of Ray Core, AIR hides lower-level abstractions and\n",
    "provides an intuitive API that was inspired by common patterns from tools such as\n",
    "scikit-learn.\n",
    "\n",
    "At its core, Ray AIR was built for both data scientists and ML engineers alike. As\n",
    "a data scientist, you can use it to build and scale your end-to-end experiments or\n",
    "individual subtasks such as preprocessing, training, tuning, scoring, or serving of ML\n",
    "models. As an ML engineer, you can go so far as to build a custom ML platform on\n",
    "top of AIR or simply leverage its unified API to integrate it with other libraries from\n",
    "your ecosystem. And Ray always gives you the flexibility to drop down and delve into\n",
    "the lower-level Ray Core API."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As part of the Ray ecosystem, AIR can leverage all its benefits, which includes a\n",
    "seamless transition from experimentation on a laptop to production workflows on a\n",
    "cluster. You often see data science teams “hand over” their ML code to teams responsible\n",
    "for production systems. In practice this can be expensive and time-consuming,\n",
    "as this process often involves modifying or even rewriting parts of the code. As we\n",
    "will see, Ray AIR helps you with this transition because AIR takes care of concerns\n",
    "such as scalability, reliability, and robustness for you.\n",
    "\n",
    "Ray AIR already has a respectable number of integrations today, but it’s also fully\n",
    "extensible. And as we will show you in the next section, its unified API provides a\n",
    "smooth workflow that allows you to drop-in-replace many of its components. For\n",
    "instance, you can use the same interface to define an XGBoost or PyTorch Trainer\n",
    "with AIR, which makes experimentation with various ML models convenient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the same time, by choosing AIR you can avoid the problem of working with\n",
    "several (distributed) systems and writing glue code for them that’s difficult to deal\n",
    "with. Teams working with many moving parts often experience rapid deprecation\n",
    "of integrations and a high maintenance burden. These issues can lead to migration\n",
    "fatigue, a reluctance to adopt new ideas due to the anticipated complexity of system\n",
    "changes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Workloads to run with AIR\n",
    "\n",
    "Now that we’ve seen examples of AIR and its fundamental concepts, let’s zoom out\n",
    "a little and discuss in principle which kinds of workloads you can run with it. We’ve\n",
    "tackled all of these workloads already throughout the book, but it’s good to recap\n",
    "them systematically. As the name suggests, AIR is built to capture common tasks in\n",
    "AI projects. These tasks can be roughly classified in the following way:\n",
    "\n",
    "- Stateless computation: Tasks like preprocessing data or computing model predictions on a batch of data\n",
    "    are stateless. Stateless workloads can be computed independently in parallel.\n",
    "    If you recall our treatment of Ray tasks from Chapter 2, stateless computation\n",
    "    is exactly what they were built for. AIR primarily uses Ray tasks for stateless\n",
    "    workloads. Many big data processing tools fall into this category.\n",
    "- Stateful computation: In contrast, model training and hyperparameter tuning are stateful operations, as\n",
    "    they update the model state during their respective training procedure. Updating\n",
    "    stateful workers in such distributed training is a difficult topic that Ray handles\n",
    "    for you. AIR uses Ray actors for stateful computations.\n",
    "- Composite workloads: Combining stateless and stateful computation, for instance by first processing\n",
    "    features and then training a model, is quite common in AI workloads. In fact,\n",
    "    it’s rare for end-to-end projects to exclusively use one or the other. Running such\n",
    "    advanced composite workloads in a distributed fashion can be described as big\n",
    "    data training, and AIR is built to handle both the stateless and stateful parts efficiently.\n",
    "- Online serving: Lastly, AIR is built to handle scalable online serving of (multiple) models. The\n",
    "    transition from the previous three workloads to serving is frictionless by design,\n",
    "    as you still operate within the same AIR ecosystem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use these types of workloads in different scenarios, too. For instance, you can\n",
    "use AIR to replace and scale out a single component of an existing pipeline. Or you\n",
    "can create your own end-to-end machine learning apps with AIR.\n",
    "You can even use AIR to build your own AI platform, as we will see later.\n",
    "\n",
    "![AIR Workloads](./images/AIR_workloads.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Key Components of Ray AIR\n",
    "\n",
    "AIR’s design philosophy is to provide you with the ability to tackle your ML workloads\n",
    "in a single script, run by a single system.\n",
    "\n",
    "\n",
    "### Datasets and Preprocessors\n",
    "\n",
    "The standard way to load data in Ray AIR is with Ray Datasets. AIR Preprocessors are\n",
    "used to transform input data into features for ML experiments.\n",
    "Since these preprocessors operate on Datasets and leverage the Ray ecosystem, they\n",
    "allow you to scale your preprocessing steps efficiently. During training an AIR Preprocessor\n",
    "is fitted to the specified training data and can then later be used for both\n",
    "training and serving. AIR comes packaged with many common preprocessors that\n",
    "cover many use cases. If you don’t find the one you need, you can easily define a\n",
    "custom preprocessor on your own.\n",
    "\n",
    "![AIR Data](./images/preprocessor_table.png)\n",
    "\n",
    "### Trainers\n",
    "\n",
    "Once you have your training and test datasets ready and your preprocessors defined,\n",
    "you can move on to specifying a Trainer that runs an ML algorithm on your data.\n",
    "Trainers provide a consistent wrapper for training frameworks such as TensorFlow, PyTorch, or\n",
    "HuggingFace. In this example we’ll focus on the latter, but it’s important to note that\n",
    "all other framework integrations work exactly the same way in terms of the Ray AIR\n",
    "API.\n",
    "\n",
    "Trainers provide scalable ML training that operates on AIR Datasets and preprocessors.\n",
    "On top of that, they’re also built to integrate well with Ray Tune for HPO, as\n",
    "we’ll see next.\n",
    "To summarize this section, the following figure shows how AIR Trainers fit ML models on\n",
    "Ray Datasets given preprocessors and a scaling configuration.\n",
    "\n",
    "![AIR Trainers](./images/AIR_trainer.png)\n",
    "\n",
    "### Tuners and Checkpoints\n",
    "\n",
    "Tuners, introduced with Ray 2.0 as part of AIR, offer scalable hyperparameter tuning\n",
    "through Ray Tune. Tuners work seamlessly with AIR Trainers, but also support arbitrary\n",
    "training functions. In our example, instead of calling fit() on your trainer\n",
    "instance from the previous section, you can pass your trainer into a Tuner. To do\n",
    "so, a Tuner needs to be instantiated with a parameter space to search over, a so-called\n",
    "TuneConfig. This config has all Tune-specific configurations like the metric you\n",
    "want to optimize and an optional RunConfig that lets you configure runtime-specific\n",
    "aspects such as the log verbosity of your Tune run.\n",
    "\n",
    "Whenever you run AIR Trainers or Tuners, they generate framework-specific Checkpoints.\n",
    "You can use these checkpoints to load models for usage across several AIR\n",
    "libraries, such as Tune, Train, or Serve. You can get a Checkpoint by accessing the\n",
    "result of a .fit() call on either a Trainer or a Tuner.\n",
    "\n",
    "Having checkpoints is great because they’re AIR’s native model exchange format.\n",
    "You can also use them to pick up trained models at a later stage, without having\n",
    "to worry about custom ways to store and load the models in question. Figure 10-3\n",
    "schematically shows how AIR Tuners work with AIR Trainers.\n",
    "\n",
    "![AIR Trainers](./images/AIR_tuner.png)\n",
    "\n",
    "### Running batch prediction\n",
    "\n",
    "TODO: this needs to be adapted for new \"map_batches\" paradigm\n",
    "\n",
    "![AIR Batch Inference](./images/AIR_predictor.png)\n",
    "\n",
    "### Online Serving Deployments\n",
    "\n",
    "Instead of using batch prediction and interacting with the model in question\n",
    "directly, you can leverage Ray Serve to deploy an inference service that you can\n",
    "query over HTTP. You do that by using the PredictorDeployment class and deploy\n",
    "it using our checkpoint.\n",
    "\n",
    "![AIR Deployments](./images/AIR_deployment.png)\n",
    "\n",
    "Here's an overview of all components at once:\n",
    "\n",
    "![AIR Overview](./images/air_plan.png)\n",
    "\n",
    "\n",
    "It’s important to stress again that we’ve been using a single Python script for this\n",
    "example and a single distributed system in Ray AIR to do all the heavy lifting. In\n",
    "fact, you can use this example script and scale it out to a large cluster that uses CPUs\n",
    "for preprocessing and GPUs for training and separately configure the deployment\n",
    "simply by modifying the parameters of the scaling configuration and similar options\n",
    "in that script. This isn’t as easy or common as it may seem, and it is not unusual\n",
    "for data scientists to have to use multiple frameworks (e.g., one for data loading and\n",
    "processing, one for training, and one for serving)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: You can also use Ray AIR with RLlib, but the integration is still in\n",
    "its early stages. For instance, to integrate RLlib with AIR Trainers,\n",
    "you’d use the RLTrainer that allows you to pass in all arguments\n",
    "that you’d pass to a standard RLlib algorithm. After training, you\n",
    "can store the resulting RL model in an AIR Checkpoint, just as\n",
    "with any other AIR Trainer. To deploy your trained RL model,\n",
    "you can use Serve’s PredictorDeployment class by passing your\n",
    "checkpoint along with the RLPredictor class.\n",
    "This API might be subject to change, but you can see an example of\n",
    "how this works in the AIR documentation.\n",
    "\n",
    "TODO: maybe 10.9 on distributed model training can be interesting for examples?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An Example of Training and Deploying Large Language Models with AIR\n",
    "\n",
    "\n",
    "In this example, we will showcase how to use the Ray AIR for fine-tuning GPT-J.\n",
    "GPT-J is a GPT-2-like causal language model trained on the Pile dataset.\n",
    "This particular model has 6 billion parameters.\n",
    "For more information on GPT-J, click [here](https://huggingface.co/docs/transformers/model_doc/gptj).\n",
    "\n",
    "We will use Ray AIR (with the 🤗 Transformers integration) and a pretrained model from Hugging Face hub. Note that you can easily adapt this example to use other similar models.\n",
    "\n",
    "This example focuses more on the performance and distributed computing aspects of Ray AIR. If you are looking for a more beginner-friendly introduction to Ray AIR 🤗 Transformers integration, see /ray-air/examples/huggingface_text_classification.\n",
    "\n",
    "It is highly recommended to read [Ray AIR Key Concepts](TODO) and [Ray Data Key Concepts](TODO) before starting this example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up Ray\n",
    "\n",
    "We will use `ray.init()` to initialize a local cluster.\n",
    "By default, this cluster will consist of only the machine you are running this notebook on.\n",
    "You can also run this notebook on an Anyscale cluster.\n",
    "\n",
    "We define a {ref}`runtime environment <runtime-environments>` to ensure that the Ray workers have access to all the necessary packages. You can omit the `runtime_env` argument if you have all the packages already installed on each node in your cluster.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage stats collection is enabled by default for nightly wheels. To disable this, run the following command: `ray disable-usage-stats` before starting Ray. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 15:52:44,494\tINFO worker.py:1554 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265 \u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.7.16', ray_version='3.0.0.dev0', ray_commit='862427326b5459b252eb82ad87dc08d9b1db60f2', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-05-17_15-52-41_646499_96237/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-05-17_15-52-41_646499_96237/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2023-05-17_15-52-41_646499_96237', 'metrics_export_port': 57303, 'gcs_address': '127.0.0.1:56548', 'address': '127.0.0.1:56548', 'dashboard_agent_listen_port': 52365, 'node_id': '3f478892ea03901c239b6d97d3025953addad4509fc8e10d7506dd64'})",
      "text/html": "<div>\n    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n            <g id=\"layer-1\">\n                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n            </g>\n        </svg>\n        <table>\n            <tr>\n                <td style=\"text-align: left\"><b>Python version:</b></td>\n                <td style=\"text-align: left\"><b>3.7.16</b></td>\n            </tr>\n            <tr>\n                <td style=\"text-align: left\"><b>Ray version:</b></td>\n                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n            </tr>\n            <tr>\n    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n</tr>\n\n        </table>\n    </div>\n</div>\n"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"pip\": [\n",
    "            \"datasets\",\n",
    "            \"evaluate\",\n",
    "            \"accelerate>=0.16.0\",\n",
    "            \"transformers>=4.26.0\",\n",
    "            \"torch>=1.12.0\",\n",
    "            \"deepspeed\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "We will be fine-tuning the model on the [`tiny_shakespeare` dataset](https://huggingface.co/datasets/tiny_shakespeare), comprised of 40,000 lines of Shakespeare from a variety of Shakespeare's plays. The aim will be to make the GPT-J model better at generating text in the style of Shakespeare."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading tiny_shakespeare dataset\")\n",
    "current_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "current_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use [Ray Data](data) for distributed preprocessing and data ingestion. We can easily convert the dataset obtained from Hugging Face Hub to Ray Data by using {meth}`ray.data.from_huggingface`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ray.data\n",
    "\n",
    "ray_datasets = ray.data.from_huggingface(current_dataset)\n",
    "ray_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the dataset is represented by a single large string, we will need to do some preprocessing. For that, we will define two [Ray AIR Preprocessors](air-preprocessors) using the {class}`~ray.data.preprocessors.BatchMapper` API, allowing us to define functions that will be applied on batches of data.\n",
    "\n",
    "The `split_text` function will take the single string and split it into separate lines, removing empty lines and character names ending with ':' (eg. 'ROMEO:'). The `tokenize` function will take the lines and tokenize them using the 🤗 Tokenizer associated with the model, ensuring each entry has the same length (`block_size`) by padding and truncating. This is necessary for training.\n",
    "\n",
    "```{note}\n",
    "This preprocessing can be done in other ways. A common pattern is to tokenize first, and then split the obtained tokens into equally-sized blocks.\n",
    "```\n",
    "\n",
    "We will use the `splitter` and `tokenizer` Preprocessors below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ray.data.preprocessors import BatchMapper\n",
    "\n",
    "block_size = 512\n",
    "\n",
    "def split_text(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    text = list(batch[\"text\"])\n",
    "    flat_text = \"\".join(text)\n",
    "    split_text = [\n",
    "        x.strip()\n",
    "        for x in flat_text.split(\"\\n\")\n",
    "        if x.strip() and not x.strip()[-1] == \":\"\n",
    "    ]\n",
    "    return pd.DataFrame(split_text, columns=[\"text\"])\n",
    "\n",
    "\n",
    "def tokenize(batch: pd.DataFrame) -> dict:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"text\"]),\n",
    "        truncation=True,\n",
    "        max_length=block_size,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "splitter = BatchMapper(split_text, batch_format=\"pandas\")\n",
    "tokenizer = BatchMapper(tokenize, batch_format=\"pandas\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-Tuning and Hyperparameter Optimization\n",
    "\n",
    "We can now configure Ray AIR's {class}`~ray.train.hf_transformers.TransformersTrainer` to perform distributed fine-tuning of the model. In order to do that, we specify a `trainer_init_per_worker` function, which creates a 🤗 Transformers `Trainer` that will be distributed by Ray using Distributed Data Parallelism (using PyTorch Distributed backend internally). This means that each worker will have its own copy of the model, but operate on different data, At the end of each step, all the workers will sync gradients.\n",
    "\n",
    "Because GPT-J is a relatively large model, it may not be possible to fit it on smaller GPU types (<=16 GB GRAM). To deal with that issue, we can use [DeepSpeed](https://github.com/microsoft/DeepSpeed), a library to optimize the training process and allow us to (among other things) offload and partition optimizer and parameter states, reducing GRAM usage. Furthermore, DeepSpeed ZeRO Stage 3 allows us to load large models without running out of memory.\n",
    "\n",
    "🤗 Transformers and Ray AIR's integration ({class}`~ray.train.hf_transformers.TransformersTrainer`) allow you to easily configure and use DDP and DeepSpeed. All you need to do is specify the DeepSpeed configuration in the [`TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) object.\n",
    "\n",
    "```{tip}\n",
    "There are many DeepSpeed settings that allow you to trade-off speed for memory usage. The settings used below are tailored to the cluster setup used (16 g4dn.4xlarge nodes) and per device batch size of 16. Some things to keep in mind:\n",
    "- If your GPUs support bfloat16, use that instead of float16 mixed precision to get better performance and prevent overflows. Replace `fp16=True` with `bf16=True` in `TrainingArguments`.\n",
    "- If you are running out of GRAM: try reducing batch size (defined in the cell below the next one), set `\"overlap_comm\": False` in DeepSpeed config.\n",
    "- If you are running out of RAM, add more nodes to your cluster, use nodes with more RAM, set `\"pin_memory\": False` in the DeepSpeed config, reduce the batch size, and remove `\"offload_param\"` from the DeepSpeed config.\n",
    "\n",
    "For more information on DeepSpeed configuration, refer to [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/deepspeed) and [DeepSpeed documentation](https://www.deepspeed.ai/docs/config-json/).\n",
    "\n",
    "Additionally, if you prefer a lower-level API, the logic below can be expressed as an [Accelerate training loop](https://github.com/huggingface/accelerate/blob/main/examples/by_feature/deepspeed_with_config_support.py) distributed by a Ray AIR {class}`~ray.train.torch.torch_trainer.TorchTrainer`.\n",
    "```\n",
    "\n",
    "#### Training speed\n",
    "\n",
    "As we are using data parallelism, each worker operates on its own shard of the data. The batch size set in `TrainingArguments` is the **per device batch size** (per worker batch size). By changing the number of workers, we can change the **effective batch size** and thus the time needed for training to complete. The effective batch size is then calculated as `per device batch size * number of workers * number of gradient accumulation steps`. As we add more workers, the effective batch size rises and thus we need less time to complete a full epoch. While the speedup is not exactly linear due to extra communication overheads, in many cases it can be close to linear.\n",
    "\n",
    "The preprocessed dataset has 1348 examples. We have set per device batch size to 16.\n",
    "\n",
    "* With 16 g4dn.4xlarge nodes, the effective batch size was 256, which equals to 85 steps per epoch. One epoch took **~2440 seconds** (including initialization time).\n",
    "\n",
    "* With 32 g4dn.4xlarge nodes, the effective batch size was 512, which equals to 43 steps per epoch. One epoch took **~1280 seconds** (including initialization time)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import (\n",
    "    GPTJForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from transformers.utils.logging import disable_progress_bar, enable_progress_bar\n",
    "import torch\n",
    "\n",
    "from ray.air import session\n",
    "\n",
    "\n",
    "def trainer_init_per_worker(train_dataset, eval_dataset=None, **config):\n",
    "    # Use the actual number of CPUs assigned by Ray\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(\n",
    "        session.get_trial_resources().bundles[-1].get(\"CPU\", 1)\n",
    "    )\n",
    "    # Enable tf32 for better performance\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    batch_size = config.get(\"batch_size\", 4)\n",
    "    epochs = config.get(\"epochs\", 2)\n",
    "    warmup_steps = config.get(\"warmup_steps\", 0)\n",
    "    learning_rate = config.get(\"learning_rate\", 0.00002)\n",
    "    weight_decay = config.get(\"weight_decay\", 0.01)\n",
    "\n",
    "    deepspeed = {\n",
    "        \"fp16\": {\n",
    "            \"enabled\": \"auto\",\n",
    "            \"initial_scale_power\": 8,\n",
    "        },\n",
    "        \"bf16\": {\"enabled\": \"auto\"},\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": \"auto\",\n",
    "                \"betas\": \"auto\",\n",
    "                \"eps\": \"auto\",\n",
    "            },\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True,\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"gather_16bit_weights_on_model_save\": True,\n",
    "            \"round_robin_gradients\": True,\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": 10,\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False,\n",
    "    }\n",
    "\n",
    "    print(\"Preparing training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        \"output\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"no\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        label_names=[\"input_ids\", \"attention_mask\"],\n",
    "        num_train_epochs=epochs,\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        deepspeed=deepspeed,\n",
    "    )\n",
    "    disable_progress_bar()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Loading model\")\n",
    "\n",
    "    model = GPTJForCausalLM.from_pretrained(model_name, use_cache=False)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    enable_progress_bar()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "    return trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With our `trainer_init_per_worker` complete, we can now instantiate the {class}`~ray.train.hf_transformers.TransformersTrainer`. Aside from the function, we set the `scaling_config`, controlling the amount of workers and resources used, and the `datasets` we will use for training and evaluation.\n",
    "\n",
    "We pass the preprocessors we have defined earlier as an argument, wrapped in a {class}`~ray.data.preprocessors.chain.Chain`. The preprocessor will be included with the returned {class}`~ray.air.checkpoint.Checkpoint`, meaning it will also be applied during inference.\n",
    "\n",
    "```{note}\n",
    "If you want to upload checkpoints to cloud storage (eg. S3), set {class}`air.RunConfig(storage_path) <ray.air.RunConfig>`. See {ref}`train-run-config` for an example. Using cloud storage is highly recommended, especially for production.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray.train.hf_transformers import TransformersTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.data.preprocessors import Chain\n",
    "\n",
    "\n",
    "trainer = TransformersTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    trainer_init_config={\n",
    "        \"batch_size\": 16,  # per device\n",
    "        \"epochs\": 1,\n",
    "    },\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=num_workers,\n",
    "        use_gpu=use_gpu,\n",
    "        resources_per_worker={\"GPU\": 1, \"CPU\": cpus_per_worker},\n",
    "    ),\n",
    "    datasets={\"train\": ray_datasets[\"train\"], \"evaluation\": ray_datasets[\"validation\"]},\n",
    "    preprocessor=Chain(splitter, tokenizer),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we call the {meth}`~ray.train.hf_transformers.TransformersTrainer.fit` method to start training with Ray AIR. We will save the {class}`~ray.air.Result` object to a variable so we can access metrics and checkpoints."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use the returned {class}`~ray.air.Result` object to access metrics and the Ray AIR {class}`~ray.air.checkpoint.Checkpoint` associated with the last iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = results.checkpoint\n",
    "checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate text from prompt\n",
    "\n",
    "We can use the {class}`~ray.train.hf_transformers.huggingface_predictor.TransformersPredictor` to generate predictions from our fine-tuned model.\n",
    "\n",
    "```{tip}\n",
    "For large scale batch inference, consider configuring cloud checkpointing and then pass the cloud-backed {class}`~ray.air.checkpoint.Checkpoint` to {class}`~ray.train.batch_predictor.BatchPredictor`. More information [here](air-predictors).\n",
    "```\n",
    "\n",
    "Because the {class}`~ray.train.hf_transformers.huggingface_predictor.TransformersPredictor` uses a 🤗 Transformers [`pipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines) under the hood, we disable the tokenizer AIR Preprocessor we have used for training and let the `pipeline` to tokenize the data itself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint.set_preprocessor(None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also set `device_map=\"auto\"` so that the model is automatically placed on the right device and set the `task` to `\"text-generation\"`. The `predict` method passes the arguments to a 🤗 Transformers `pipeline` call."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ray.train.hf_transformers import TransformersPredictor\n",
    "import pandas as pd\n",
    "\n",
    "prompts = pd.DataFrame([\"Romeo and Juliet\", \"Romeo\", \"Juliet\"], columns=[\"text\"])\n",
    "\n",
    "# Predict on the head node.\n",
    "predictor = TransformersPredictor.from_checkpoint(\n",
    "    checkpoint=checkpoint,\n",
    "    task=\"text-generation\",\n",
    "    torch_dtype=torch.float16 if use_gpu else None,\n",
    "    device_map=\"auto\",\n",
    "    use_gpu=use_gpu,\n",
    ")\n",
    "prediction = predictor.predict(\n",
    "    prompts,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    min_length=32,\n",
    "    max_length=128,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running Batch Inference\n",
    "\n",
    "TODO intro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/gpt-j-6B\"\n",
    "revision = \"float16\"  # use float16 weights to fit in 16GB GPUs\n",
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purposes of this example, we will use a very small toy dataset composed of multiple copies of our prompt. Ray Data can handle much bigger datasets with ease."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ray.data\n",
    "import pandas as pd\n",
    "\n",
    "ds = ray.data.from_pandas(pd.DataFrame([prompt] * 10, columns=[\"prompt\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we will be using a pretrained model from Hugging Face hub, the simplest way is to use {meth}`map_batches <ray.data.Dataset.map_batches>` with a [callable class UDF](transforming_data_actors). This will allow us to save time by initializing a model just once and then feed it multiple batches of data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PredictCallable:\n",
    "    def __init__(self, model_id: str, revision: str = None):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        import torch\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            revision=revision,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\",  # automatically makes use of all GPUs available to the Actor\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        tokenized = self.tokenizer(\n",
    "            list(batch[\"prompt\"]), return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokenized.input_ids.to(self.model.device)\n",
    "        attention_mask = tokenized.attention_mask.to(self.model.device)\n",
    "\n",
    "        gen_tokens = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            max_length=100,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(gen_tokens), columns=[\"responses\"]\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All that is left is to run the `map_batches` method on the dataset. We specify that we want to use one GPU for each Ray Actor that will be running our callable class.\n",
    "\n",
    "Also notice that we repartition the dataset into 100 partitions before mapping batches. This is to make sure there will be enough parallel tasks to take advantage of all the GPUs. 100 is an arbitrary number. You can pick any other numbers as long as it is more than the number of available GPUs in the cluster.\n",
    "\n",
    "```{tip}\n",
    "If you have access to large GPUs, you may want to increase the batch size to better saturate them.\n",
    "\n",
    "If you want to use inter-node model parallelism, you can also increase `num_gpus`. As we have created the model with `device_map=\"auto\"`, it will be automatically placed on correct devices. Note that this requires nodes with multiple GPUs.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = (\n",
    "    ds\n",
    "    .repartition(100)\n",
    "    .map_batches(\n",
    "        PredictCallable,\n",
    "        batch_size=4,\n",
    "        fn_constructor_kwargs=dict(model_id=model_id, revision=revision),\n",
    "        batch_format=\"pandas\",\n",
    "        compute=ray.data.ActorPoolStrategy(),\n",
    "        num_gpus=1,\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After `map_batches` is done, we can view our generated text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds.take_all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may notice that we are not using an AIR {class}`Predictor <ray.train.predictor.Predictor>` here. This is because Predictors are mainly intended to be used with AIR {class}`Checkpoints <ray.air.checkpoint.Checkpoint>`, which we don't for this example. See {ref}`air-predictors` for more information and usage examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running Online Model Serving\n",
    "\n",
    "Setting up basic serving with Ray Serve is very similar to {doc}`batch inference with Ray Data </ray-air/examples/gptj_batch_prediction>`. First, we define a callable class that will serve as the [Serve deployment](serve-key-concepts-deployment). At runtime, a deployment consists of a number of *replicas*, which are individual copies of the class or function that are started in separate Ray Actors (processes). The number of replicas can be scaled up or down (or even autoscaled) to match the incoming request load.\n",
    "\n",
    "We make sure to set the deployment to use 1 GPU by setting `\"num_gpus\"` in `ray_actor_options`. We load the model in `__init__`, which will allow us to save time by initializing a model just once and then use it to handle multiple requests.\n",
    "\n",
    "```{tip}\n",
    "If you want to use inter-node model parallelism, you can also increase `num_gpus`. As we have created the model with `device_map=\"auto\"`, it will be automatically placed on correct devices. Note that this requires nodes with multiple GPUs.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class PredictDeployment:\n",
    "    def __init__(self, model_id: str, revision: str = None):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        import torch\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            revision=revision,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\",  # automatically makes use of all GPUs available to the Actor\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def generate(self, text: str) -> pd.DataFrame:\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids.to(\n",
    "            self.model.device\n",
    "        )\n",
    "\n",
    "        gen_tokens = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            max_length=100,\n",
    "        )\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(gen_tokens), columns=[\"responses\"]\n",
    "        )\n",
    "\n",
    "    async def __call__(self, http_request: Request) -> str:\n",
    "        json_request: str = await http_request.json()\n",
    "        prompts = []\n",
    "        for prompt in json_request:\n",
    "            text = prompt[\"text\"]\n",
    "            if isinstance(text, list):\n",
    "                prompts.extend(text)\n",
    "            else:\n",
    "                prompts.append(text)\n",
    "        return self.generate(prompts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now `bind` the deployment with our arguments, and use {meth}`~ray.serve.run` to start it.\n",
    "\n",
    "```{note}\n",
    "If you were running this script outside of a Jupyter notebook, the recommended way is to use the [`serve run` CLI command](serve-cli). In this case, you would remove the `serve.run(deployment)` line, and instead start the deployment by calling `serve run FILENAME:deployment`.\n",
    "\n",
    "For more information, see [Serve Development Workflow](serve-dev-workflow).\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\n",
    "serve.run(deployment)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try submitting a request to our deployment. We will use the same prompt as before, and send a POST request. The deployment will generate a response and return it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")\n",
    "\n",
    "sample_input = {\"text\": prompt}\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This wraps up our extended LLM examples.\n",
    "Let's next take a look at Ray AIR's ecosystem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An Overview of Ray AIR Integrations\n",
    "\n",
    "Next, we'll show you the full extent of integrations currently available for Ray.\n",
    "We do so by discussing this ecosystem as seen from Ray AIR so that we can discuss\n",
    "it in the context of a representative AIR workflow.\n",
    "Clearly, we simply can’t give you examples for all the libraries in Ray’s ecosystem.\n",
    "Where appropriate, we’ll point you to more advanced resources to deepen your understanding.\n",
    "\n",
    "![AIR Data Table](./images/data_eco_table.png)\n",
    "\n",
    "![AIR Train Table](./images/training_eco_table.png)\n",
    "\n",
    "![AIR Tune Table](./images/tune_eco_table.png)\n",
    "\n",
    "![AIR Serve Table](./images/serve_eco_table.png)\n",
    "\n",
    "### An Overview of Ray’s Integrations\n",
    "\n",
    "Let’s summarize all the integrations mentioned in this chapter (and throughout the\n",
    "book) in one concise diagram. In the following figure we list all integrations currently available:\n",
    "\n",
    "![AIR Eco](./images/Ray_extended_eco.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How AIR compares to related systems\n",
    "\n",
    "Now that you know much more about Ray and its libraries, this chapter is also the\n",
    "right place to compare what Ray offers to similar systems. As you’ve seen, Ray’s ecosystem\n",
    "is quite complex, can be seen from different angles, and is used for different\n",
    "purposes. That means many aspects of Ray can be compared to other tools in the\n",
    "market. We’ll also comment on how to integrate Ray into more complex workflows in existing\n",
    "ML platforms.\n",
    "\n",
    "We’ve not made any direct comparisons with other systems up to this point, for the\n",
    "simple reason that it makes little sense to compare Ray to something if you don’t\n",
    "have a good grasp of what Ray is yet. As Ray is quite flexible and comes with a lot\n",
    "of components, it can be compared to different types of tools in the broader ML\n",
    "ecosystem.\n",
    "Let’s start with a comparison of the more obvious candidates, namely, Python-based\n",
    "frameworks for cluster computing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distributed Python Frameworks\n",
    "\n",
    "If you consider frameworks for distributed computing that offer full Python support\n",
    "and don’t lock you into any cloud offering, the current “big three” are Dask, Spark,\n",
    "and Ray. While there are certain technical and context-dependent performance differences\n",
    "between these frameworks, it’s best to compare them in terms of the workloads\n",
    "you want to run on them. Table XXX compares the most common workload\n",
    "types:\n",
    "\n",
    "![AIR Dask Spark Table](./images/dask_spark_ray_table.png)\n",
    "\n",
    "### Ray AIR and the Broader ML Ecosystem\n",
    "\n",
    "Ray AIR focuses primarily on AI compute, for instance by providing any kind of\n",
    "distributed training via Ray Train, but it’s not built to cover every aspect of an AI\n",
    "workload. For instance, AIR chooses to integrate with tracking and monitoring tools\n",
    "for ML experiments, as well as with data storage solutions, rather than providing\n",
    "native solutions.\n",
    "\n",
    "On the other side of the spectrum, you can find categories of tools for which Ray AIR\n",
    "can be considered an alternative. For instance, there are many framework-specific\n",
    "toolkits such as TorchX or TFX that tie in tightly with their respective frameworks. In\n",
    "contrast, AIR is framework-agnostic, thereby preventing vendor lock-in, and offers\n",
    "similar tooling.\n",
    "\n",
    "It’s also interesting to briefly touch on how Ray AIR compares to specific cloud offerings.\n",
    "Some major cloud services offer comprehensive toolkits to tackle ML workloads\n",
    "in Python. To name just one, AWS Sagemaker is a great all-in-one package that\n",
    "allows you to connect well with your AWS ecosystem. AIR does not aim to replace\n",
    "tools like SageMaker. Instead, it aims to provide alternatives for compute-intensive\n",
    "components like training, evaluation, and serving.\n",
    "\n",
    "AIR also represents a valid alternative to ML workflow frameworks such as KubeFlow\n",
    "or Flyte. In contrast to many container-based solutions, AIR offers an intuitive,\n",
    "high-level Python API and offers native support for distributed data.\n",
    "\n",
    "Sometimes the situation is not as clear-cut, and Ray AIR can be seen or used as both\n",
    "an alternative or a complementary component in the ML ecosystem.\n",
    "For instance, as open source systems, Ray and AIR in particular can be used within\n",
    "hosted ML platforms such as SageMaker, but you can also build your own ML\n",
    "Platforms with it. Also, as mentioned, AIR can’t always compete with dedicated big\n",
    "data processing systems like Spark or Dask, but often Ray Datasets can be enough to\n",
    "suit your processing needs.\n",
    "\n",
    "As we mentioned earlier, it is central to AIR’s design philosophy to have\n",
    "the ability to express your ML workloads in a single script and execute it on Ray\n",
    "as a single distributed system. Since Ray handles all the task placement and execution\n",
    "on your cluster for you under the hood, there’s usually no need to explicitly\n",
    "orchestrate your workloads (or stitch together many complex distributed systems).\n",
    "Of course, this philosophy should not be taken too literally—sometimes you need\n",
    "multiple systems or to split up tasks into several stages. On the other hand, dedicated\n",
    "workflow orchestration tools like Argo or AirFlow can be very useful when used in\n",
    "a complementary fashion. For instance, you might want to run Ray as a step in the\n",
    "Lightning MLOps framework.\n",
    "\n",
    "\n",
    "### How to Integrate AIR into Your ML Platform\n",
    "\n",
    "Now that you have a deeper understanding of the relationship of Ray, and AIR in\n",
    "particular, to other ecosystem components, let’s summarize what it takes to build your\n",
    "own ML platform and integrate Ray with other ecosystem components.\n",
    "\n",
    "The core of your ML system build with AIR consists of a set of Ray Clusters, each\n",
    "responsible for different jobs. For instance, one cluster might run preprocessing, train\n",
    "a PyTorch model, and run inference; another one might simply pick up previously\n",
    "trained models for batch inference and model serving, and so on. You can leverage\n",
    "the Ray Autoscaler to fulfill your scaling needs and could deploy the whole system\n",
    "on Kubernetes with KubeRay. You can then augment this core system with other\n",
    "components as you see fit, for example:\n",
    "\n",
    "- You might want to add other compute steps to your setup, such as running\n",
    "    data-intensive preprocessing tasks with Spark.\n",
    "- You can use a workflow orchestrator such as AirFlow, Oozie, or SageMaker\n",
    "    Pipelines to schedule and create your Ray Clusters and run Ray AIR apps and\n",
    "    services. Each AIR app can be part of a larger orchestrated workflow, for instance\n",
    "    by tying into a Spark ETL job from the first bullet point.\n",
    "- You can also create your Ray AIR clusters for interactive use with Jupyter notebooks,\n",
    "for instance hosted by Google Colab or Databricks Notebooks.\n",
    "- If you need access to a feature store such as Feast or Tecton, Ray Train, Datasets,\n",
    "and Serve have an integration for such tools.16\n",
    "- For experiment tracking or metric stores, Ray Train and Tune provide integration\n",
    "    with tools such as MLflow and Weights & Biases.\n",
    "- You can also retrieve and store your data and models from external storage\n",
    "    solutions like S3, as shown.\n",
    "\n",
    "![AIR ML Platform Table](./images/AIR_ML_platform.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter you’ve seen how all the Ray libraries we’ve introduced come together\n",
    "to form the Ray AI Runtime. You’ve learned about all the key concepts that allow\n",
    "you to build scalable ML projects, from experimentation to production. In particular,\n",
    "you’ve seen how Ray Datasets are used for stateless computations such as feature\n",
    "preprocessing, and how Ray Train and Tune are used for stateful computations\n",
    "such as model training. Seamlessly combining these types of computations in\n",
    "complex AI workloads and scaling them out to large clusters is a key strength of AIR.\n",
    "Deploying your AIR projects comes essentially for free, as AIR fully integrates with\n",
    "Ray Serve as well.\n",
    "\n",
    "You also learned about Ray AIR’s ecosystem.\n",
    "You should now be able to go out there and run your own AIR experiments,\n",
    "together with all the tools you’re already using or intend to use in the future. We’ve\n",
    "also discussed Ray’s limits, how it compares to various related systems, and how you\n",
    "can use Ray with other tools to augment or build out your own ML platforms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

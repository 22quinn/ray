{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba89df43",
   "metadata": {},
   "source": [
    "# An Introduction to the Ray AI Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900de5b",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab TODO](https://colab.research.google.com/github/XXX).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/XXX\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "TODO: Make this 2.5 or 2.6 later.\n",
    "The book has been written for Ray 2.4.0, which you can install using `pip install ray==2.4.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f0182",
   "metadata": {},
   "source": [
    "To run the examples for this chapter, you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8477db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO pin all versions here\n",
    "! pip install \"ray[air]>=2.4.0\" \"accelerate>=0.16.0\" \"transformers>=4.26.0\"\n",
    "! pip install \"numpy<1.24\" \"torch>=1.12.0\" \"datasets\" \"evaluate\" \"deepspeed\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "In this chapter we’ll introduce you to the core concepts of the Ray AI Runtime (AIR) and how you can\n",
    "use it to build and deploy common ML workflows. To introduce its components we’ll build\n",
    "an AIR application that fine-tunes an open-source language model, deploys it for online\n",
    "inference and uses the model for offline batch inference.\n",
    "We will also tell you when and why to use AIR and give you a brief overview of its ecosystem.\n",
    "We close with an in-depth discussion of the relationship of AIR with other systems."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why and When to Use AIR?\n",
    "\n",
    "Running ML workloads with Ray has been a constant evolution over the last couple\n",
    "of years. Ray RLlib and Tune were the first libraries built on top of Ray Core.\n",
    "Components like Ray Train, Serve, and more recently Datasets followed shortly\n",
    "after. The addition of Ray AIR as an umbrella for all other Ray ML libraries is the\n",
    "result of active discussions with and feedback from the ML community. Ray, as a\n",
    "Python-native tool with good GPU support and stateful primitives (Ray actors) for\n",
    "complex ML workloads, is a natural candidate for building a runtime like AIR."
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ray AIR is a unified toolkit for your ML workloads that offers many third-party\n",
    "integrations for model training or accessing custom data sources. In the spirit of the\n",
    "other ML libraries built on top of Ray Core, AIR hides lower-level abstractions and\n",
    "provides an intuitive API that was inspired by common patterns from tools such as\n",
    "scikit-learn.\n",
    "\n",
    "At its core, Ray AIR was built for both data scientists and ML engineers alike. As\n",
    "a data scientist, you can use it to build and scale your end-to-end experiments or\n",
    "individual subtasks such as preprocessing, training, tuning, scoring, or serving of ML\n",
    "models. As an ML engineer, you can go so far as to build a custom ML platform on\n",
    "top of AIR or simply leverage its unified API to integrate it with other libraries from\n",
    "your ecosystem. And Ray always gives you the flexibility to drop down and delve into\n",
    "the lower-level Ray Core API."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As part of the Ray ecosystem, AIR can leverage all its benefits, which includes a\n",
    "seamless transition from experimentation on a laptop to production workflows on a\n",
    "cluster. You often see data science teams “hand over” their ML code to teams responsible\n",
    "for production systems. In practice this can be expensive and time-consuming,\n",
    "as this process often involves modifying or even rewriting parts of the code. As we\n",
    "will see, Ray AIR helps you with this transition because AIR takes care of concerns\n",
    "such as scalability, reliability, and robustness for you.\n",
    "\n",
    "Ray AIR already has a respectable number of integrations today, but it’s also fully\n",
    "extensible. And as we will show you in the next section, its unified API provides a\n",
    "smooth workflow that allows you to drop-in-replace many of its components. For\n",
    "instance, you can use the same interface to define an XGBoost or PyTorch Trainer\n",
    "with AIR, which makes experimentation with various ML models convenient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the same time, by choosing AIR you can avoid the problem of working with\n",
    "several (distributed) systems and writing glue code for them that’s difficult to deal\n",
    "with. Teams working with many moving parts often experience rapid deprecation\n",
    "of integrations and a high maintenance burden. These issues can lead to migration\n",
    "fatigue, a reluctance to adopt new ideas due to the anticipated complexity of system\n",
    "changes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Workloads to run with AIR\n",
    "\n",
    "Now that we’ve seen examples of AIR and its fundamental concepts, let’s zoom out\n",
    "a little and discuss in principle which kinds of workloads you can run with it. We’ve\n",
    "tackled all of these workloads already throughout the book, but it’s good to recap\n",
    "them systematically. As the name suggests, AIR is built to capture common tasks in\n",
    "AI projects. These tasks can be roughly classified in the following way:\n",
    "\n",
    "- Stateless computation: Tasks like preprocessing data or computing model predictions on a batch of data\n",
    "    are stateless. Stateless workloads can be computed independently in parallel.\n",
    "    If you recall our treatment of Ray tasks from Chapter 2, stateless computation\n",
    "    is exactly what they were built for. AIR primarily uses Ray tasks for stateless\n",
    "    workloads. Many big data processing tools fall into this category.\n",
    "- Stateful computation: In contrast, model training and hyperparameter tuning are stateful operations, as\n",
    "    they update the model state during their respective training procedure. Updating\n",
    "    stateful workers in such distributed training is a difficult topic that Ray handles\n",
    "    for you. AIR uses Ray actors for stateful computations.\n",
    "- Composite workloads: Combining stateless and stateful computation, for instance by first processing\n",
    "    features and then training a model, is quite common in AI workloads. In fact,\n",
    "    it’s rare for end-to-end projects to exclusively use one or the other. Running such\n",
    "    advanced composite workloads in a distributed fashion can be described as big\n",
    "    data training, and AIR is built to handle both the stateless and stateful parts efficiently.\n",
    "- Online serving: Lastly, AIR is built to handle scalable online serving of (multiple) models. The\n",
    "    transition from the previous three workloads to serving is frictionless by design,\n",
    "    as you still operate within the same AIR ecosystem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use these types of workloads in different scenarios, too. For instance, you can\n",
    "use AIR to replace and scale out a single component of an existing pipeline. Or you\n",
    "can create your own end-to-end machine learning apps with AIR.\n",
    "You can even use AIR to build your own AI platform, as we will see later.\n",
    "\n",
    "![AIR Workloads](./images/AIR_workloads.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Key Components of Ray AIR\n",
    "\n",
    "AIR’s design philosophy is to provide you with the ability to tackle your ML workloads\n",
    "in a single script, run by a single system.\n",
    "\n",
    "\n",
    "### Datasets and Preprocessors\n",
    "\n",
    "The standard way to load data in Ray AIR is with Ray Datasets. AIR Preprocessors are\n",
    "used to transform input data into features for ML experiments.\n",
    "Since these preprocessors operate on Datasets and leverage the Ray ecosystem, they\n",
    "allow you to scale your preprocessing steps efficiently. During training an AIR Preprocessor\n",
    "is fitted to the specified training data and can then later be used for both\n",
    "training and serving. AIR comes packaged with many common preprocessors that\n",
    "cover many use cases. If you don’t find the one you need, you can easily define a\n",
    "custom preprocessor on your own.\n",
    "\n",
    "![AIR Data](./images/preprocessor_table.png)\n",
    "\n",
    "### Trainers\n",
    "\n",
    "Once you have your training and test datasets ready and your preprocessors defined,\n",
    "you can move on to specifying a Trainer that runs an ML algorithm on your data.\n",
    "Trainers provide a consistent wrapper for training frameworks such as TensorFlow, PyTorch, or\n",
    "HuggingFace. In this example we’ll focus on the latter, but it’s important to note that\n",
    "all other framework integrations work exactly the same way in terms of the Ray AIR\n",
    "API.\n",
    "\n",
    "Trainers provide scalable ML training that operates on AIR Datasets and preprocessors.\n",
    "On top of that, they’re also built to integrate well with Ray Tune for HPO, as\n",
    "we’ll see next.\n",
    "To summarize this section, the following figure shows how AIR Trainers fit ML models on\n",
    "Ray Datasets given preprocessors and a scaling configuration.\n",
    "\n",
    "![AIR Trainers](./images/AIR_trainer.png)\n",
    "\n",
    "### Tuners and Checkpoints\n",
    "\n",
    "Tuners, introduced with Ray 2.0 as part of AIR, offer scalable hyperparameter tuning\n",
    "through Ray Tune. Tuners work seamlessly with AIR Trainers, but also support arbitrary\n",
    "training functions. In our example, instead of calling fit() on your trainer\n",
    "instance from the previous section, you can pass your trainer into a Tuner. To do\n",
    "so, a Tuner needs to be instantiated with a parameter space to search over, a so-called\n",
    "TuneConfig. This config has all Tune-specific configurations like the metric you\n",
    "want to optimize and an optional RunConfig that lets you configure runtime-specific\n",
    "aspects such as the log verbosity of your Tune run.\n",
    "\n",
    "Whenever you run AIR Trainers or Tuners, they generate framework-specific Checkpoints.\n",
    "You can use these checkpoints to load models for usage across several AIR\n",
    "libraries, such as Tune, Train, or Serve. You can get a Checkpoint by accessing the\n",
    "result of a .fit() call on either a Trainer or a Tuner.\n",
    "\n",
    "Having checkpoints is great because they’re AIR’s native model exchange format.\n",
    "You can also use them to pick up trained models at a later stage, without having\n",
    "to worry about custom ways to store and load the models in question. Figure 10-3\n",
    "schematically shows how AIR Tuners work with AIR Trainers.\n",
    "\n",
    "![AIR Trainers](./images/AIR_tuner.png)\n",
    "\n",
    "### Running batch prediction\n",
    "\n",
    "TODO: this needs to be adapted for new \"map_batches\" paradigm\n",
    "\n",
    "![AIR Batch Inference](./images/AIR_predictor.png)\n",
    "\n",
    "### Online Serving Deployments\n",
    "\n",
    "Instead of using batch prediction and interacting with the model in question\n",
    "directly, you can leverage Ray Serve to deploy an inference service that you can\n",
    "query over HTTP. You do that by using the PredictorDeployment class and deploy\n",
    "it using our checkpoint.\n",
    "\n",
    "![AIR Deployments](./images/AIR_deployment.png)\n",
    "\n",
    "Here's an overview of all components at once:\n",
    "\n",
    "![AIR Overview](./images/air_plan.png)\n",
    "\n",
    "\n",
    "It’s important to stress again that we’ve been using a single Python script for this\n",
    "example and a single distributed system in Ray AIR to do all the heavy lifting. In\n",
    "fact, you can use this example script and scale it out to a large cluster that uses CPUs\n",
    "for preprocessing and GPUs for training and separately configure the deployment\n",
    "simply by modifying the parameters of the scaling configuration and similar options\n",
    "in that script. This isn’t as easy or common as it may seem, and it is not unusual\n",
    "for data scientists to have to use multiple frameworks (e.g., one for data loading and\n",
    "processing, one for training, and one for serving)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: You can also use Ray AIR with RLlib, but the integration is still in\n",
    "its early stages. For instance, to integrate RLlib with AIR Trainers,\n",
    "you’d use the RLTrainer that allows you to pass in all arguments\n",
    "that you’d pass to a standard RLlib algorithm. After training, you\n",
    "can store the resulting RL model in an AIR Checkpoint, just as\n",
    "with any other AIR Trainer. To deploy your trained RL model,\n",
    "you can use Serve’s PredictorDeployment class by passing your\n",
    "checkpoint along with the RLPredictor class.\n",
    "This API might be subject to change, but you can see an example of\n",
    "how this works in the AIR documentation.\n",
    "\n",
    "TODO: maybe 10.9 on distributed model training can be interesting for examples?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An Example of Training and Deploying Large Language Models with AIR\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-Tuning and Hyperparameter Optimization\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running Batch Inference\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running Online Model Serving\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An Overview of Ray AIR Integrations\n",
    "\n",
    "Next, we'll show you the full extent of integrations currently available for Ray.\n",
    "We do so by discussing this ecosystem as seen from Ray AIR so that we can discuss\n",
    "it in the context of a representative AIR workflow.\n",
    "Clearly, we simply can’t give you examples for all the libraries in Ray’s ecosystem.\n",
    "Where appropriate, we’ll point you to more advanced resources to deepen your understanding.\n",
    "\n",
    "![AIR Data Table](./images/data_eco_table.png)\n",
    "\n",
    "![AIR Train Table](./images/training_eco_table.png)\n",
    "\n",
    "![AIR Tune Table](./images/tune_eco_table.png)\n",
    "\n",
    "![AIR Serve Table](./images/serve_eco_table.png)\n",
    "\n",
    "### An Overview of Ray’s Integrations\n",
    "\n",
    "Let’s summarize all the integrations mentioned in this chapter (and throughout the\n",
    "book) in one concise diagram. In the following figure we list all integrations currently available:\n",
    "\n",
    "![AIR Eco](./images/Ray_extended_eco.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How AIR compares to related systems\n",
    "\n",
    "Now that you know much more about Ray and its libraries, this chapter is also the\n",
    "right place to compare what Ray offers to similar systems. As you’ve seen, Ray’s ecosystem\n",
    "is quite complex, can be seen from different angles, and is used for different\n",
    "purposes. That means many aspects of Ray can be compared to other tools in the\n",
    "market. We’ll also comment on how to integrate Ray into more complex workflows in existing\n",
    "ML platforms.\n",
    "\n",
    "We’ve not made any direct comparisons with other systems up to this point, for the\n",
    "simple reason that it makes little sense to compare Ray to something if you don’t\n",
    "have a good grasp of what Ray is yet. As Ray is quite flexible and comes with a lot\n",
    "of components, it can be compared to different types of tools in the broader ML\n",
    "ecosystem.\n",
    "Let’s start with a comparison of the more obvious candidates, namely, Python-based\n",
    "frameworks for cluster computing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distributed Python Frameworks\n",
    "\n",
    "If you consider frameworks for distributed computing that offer full Python support\n",
    "and don’t lock you into any cloud offering, the current “big three” are Dask, Spark,\n",
    "and Ray. While there are certain technical and context-dependent performance differences\n",
    "between these frameworks, it’s best to compare them in terms of the workloads\n",
    "you want to run on them. Table XXX compares the most common workload\n",
    "types:\n",
    "\n",
    "![AIR Dask Spark Table](./images/dask_spark_ray_table.png)\n",
    "\n",
    "### Ray AIR and the Broader ML Ecosystem\n",
    "\n",
    "Ray AIR focuses primarily on AI compute, for instance by providing any kind of\n",
    "distributed training via Ray Train, but it’s not built to cover every aspect of an AI\n",
    "workload. For instance, AIR chooses to integrate with tracking and monitoring tools\n",
    "for ML experiments, as well as with data storage solutions, rather than providing\n",
    "native solutions.\n",
    "\n",
    "On the other side of the spectrum, you can find categories of tools for which Ray AIR\n",
    "can be considered an alternative. For instance, there are many framework-specific\n",
    "toolkits such as TorchX or TFX that tie in tightly with their respective frameworks. In\n",
    "contrast, AIR is framework-agnostic, thereby preventing vendor lock-in, and offers\n",
    "similar tooling.\n",
    "\n",
    "It’s also interesting to briefly touch on how Ray AIR compares to specific cloud offerings.\n",
    "Some major cloud services offer comprehensive toolkits to tackle ML workloads\n",
    "in Python. To name just one, AWS Sagemaker is a great all-in-one package that\n",
    "allows you to connect well with your AWS ecosystem. AIR does not aim to replace\n",
    "tools like SageMaker. Instead, it aims to provide alternatives for compute-intensive\n",
    "components like training, evaluation, and serving.\n",
    "\n",
    "AIR also represents a valid alternative to ML workflow frameworks such as KubeFlow\n",
    "or Flyte. In contrast to many container-based solutions, AIR offers an intuitive,\n",
    "high-level Python API and offers native support for distributed data.\n",
    "\n",
    "Sometimes the situation is not as clear-cut, and Ray AIR can be seen or used as both\n",
    "an alternative or a complementary component in the ML ecosystem.\n",
    "For instance, as open source systems, Ray and AIR in particular can be used within\n",
    "hosted ML platforms such as SageMaker, but you can also build your own ML\n",
    "Platforms with it. Also, as mentioned, AIR can’t always compete with dedicated big\n",
    "data processing systems like Spark or Dask, but often Ray Datasets can be enough to\n",
    "suit your processing needs.\n",
    "\n",
    "As we mentioned earlier, it is central to AIR’s design philosophy to have\n",
    "the ability to express your ML workloads in a single script and execute it on Ray\n",
    "as a single distributed system. Since Ray handles all the task placement and execution\n",
    "on your cluster for you under the hood, there’s usually no need to explicitly\n",
    "orchestrate your workloads (or stitch together many complex distributed systems).\n",
    "Of course, this philosophy should not be taken too literally—sometimes you need\n",
    "multiple systems or to split up tasks into several stages. On the other hand, dedicated\n",
    "workflow orchestration tools like Argo or AirFlow can be very useful when used in\n",
    "a complementary fashion. For instance, you might want to run Ray as a step in the\n",
    "Lightning MLOps framework.\n",
    "\n",
    "\n",
    "### How to Integrate AIR into Your ML Platform\n",
    "\n",
    "Now that you have a deeper understanding of the relationship of Ray, and AIR in\n",
    "particular, to other ecosystem components, let’s summarize what it takes to build your\n",
    "own ML platform and integrate Ray with other ecosystem components.\n",
    "\n",
    "The core of your ML system build with AIR consists of a set of Ray Clusters, each\n",
    "responsible for different jobs. For instance, one cluster might run preprocessing, train\n",
    "a PyTorch model, and run inference; another one might simply pick up previously\n",
    "trained models for batch inference and model serving, and so on. You can leverage\n",
    "the Ray Autoscaler to fulfill your scaling needs and could deploy the whole system\n",
    "on Kubernetes with KubeRay. You can then augment this core system with other\n",
    "components as you see fit, for example:\n",
    "\n",
    "- You might want to add other compute steps to your setup, such as running\n",
    "    data-intensive preprocessing tasks with Spark.\n",
    "- You can use a workflow orchestrator such as AirFlow, Oozie, or SageMaker\n",
    "    Pipelines to schedule and create your Ray Clusters and run Ray AIR apps and\n",
    "    services. Each AIR app can be part of a larger orchestrated workflow, for instance\n",
    "    by tying into a Spark ETL job from the first bullet point.\n",
    "- You can also create your Ray AIR clusters for interactive use with Jupyter notebooks,\n",
    "for instance hosted by Google Colab or Databricks Notebooks.\n",
    "- If you need access to a feature store such as Feast or Tecton, Ray Train, Datasets,\n",
    "and Serve have an integration for such tools.16\n",
    "- For experiment tracking or metric stores, Ray Train and Tune provide integration\n",
    "    with tools such as MLflow and Weights & Biases.\n",
    "- You can also retrieve and store your data and models from external storage\n",
    "    solutions like S3, as shown.\n",
    "\n",
    "![AIR ML Platform Table](./images/AIR_ML_platform.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter you’ve seen how all the Ray libraries we’ve introduced come together\n",
    "to form the Ray AI Runtime. You’ve learned about all the key concepts that allow\n",
    "you to build scalable ML projects, from experimentation to production. In particular,\n",
    "you’ve seen how Ray Datasets are used for stateless computations such as feature\n",
    "preprocessing, and how Ray Train and Tune are used for stateful computations\n",
    "such as model training. Seamlessly combining these types of computations in\n",
    "complex AI workloads and scaling them out to large clusters is a key strength of AIR.\n",
    "Deploying your AIR projects comes essentially for free, as AIR fully integrates with\n",
    "Ray Serve as well.\n",
    "\n",
    "You also learned about Ray AIR’s ecosystem.\n",
    "You should now be able to go out there and run your own AIR experiments,\n",
    "together with all the tools you’re already using or intend to use in the future. We’ve\n",
    "also discussed Ray’s limits, how it compares to various related systems, and how you\n",
    "can use Ray with other tools to augment or build out your own ML platforms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
